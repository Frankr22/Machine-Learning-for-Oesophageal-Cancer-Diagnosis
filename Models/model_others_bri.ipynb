{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9cc24c2",
   "metadata": {},
   "source": [
    "# ML-diagnosis-of-esophageal-cancer\n",
    "## Supervised Machine Learning Model Selection\n",
    "Authors: \n",
    "\n",
    "Date: 2023-03-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee6ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies & Installs\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0c7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input 1\n",
    "data = pd.read_csv('../Data_Cleaned/train_test_set.csv')\n",
    "\n",
    "# Reclassified: 1 vs 2,3,4 healthy vs all\n",
    "df1= data\n",
    "df1['target'] = df1['Patient Group'].map({'BE-HGD': 1, 'EAC': 1, 'BE': 1, 'BE-ID': 1, 'BE-LGD': 1, 'NSE': 0})\n",
    "df1 = df1[df1.target<2]\n",
    "\n",
    "# Reclassified: 2 vs 3&4 BE low vs BE-HGD & EAC\n",
    "df2 = data\n",
    "df2['target'] = df2['Patient Group'].map({'BE-HGD': 1, 'EAC': 1, 'BE': 0, 'BE-ID': 0, 'BE-LGD': 0, 'NSE': 2})\n",
    "df2 = df2[df2.target<2]\n",
    "\n",
    "# Reclassified: 2 vs 3 BE low vs BE-HGD\n",
    "df3 = data\n",
    "df3['target'] = df3['Patient Group'].map({'BE-HGD': 1, 'EAC': 2, 'BE': 0, 'BE-ID': 0, 'BE-LGD': 0, 'NSE': 2})\n",
    "df3 = df3[df3.target<2]\n",
    "\n",
    "# Reclassified: 3 vs 4 BE-HGD vs AEC\n",
    "df4 = data\n",
    "df4['target'] = df4['Patient Group'].map({'BE-HGD': 0, 'EAC': 1, 'BE': 2, 'BE-ID': 2, 'BE-LGD': 2, 'NSE': 2})\n",
    "df4 = df4[df4.target<2]\n",
    "\n",
    "# Reclassified: 1&2 vs 3&4 \n",
    "df5 = data\n",
    "df5['target'] = df5['Patient Group'].map({'BE-HGD': 1, 'EAC': 1, 'BE': 0, 'BE-ID': 0, 'BE-LGD': 0, 'NSE': 0})\n",
    "df5 = df5[df5.target<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6933a3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 1 Shape:  (257, 190) (257,)\n",
      "Data 2 Shape:  (204, 190) (204,)\n",
      "Data 3 Shape:  (149, 190) (149,)\n",
      "Data 4 Shape:  (93, 190) (93,)\n",
      "Data 5 Shape:  (257, 190) (257,)\n"
     ]
    }
   ],
   "source": [
    "# Create X and y\n",
    "\n",
    "X1 = df1.drop(['Patient Group', 'target'], axis=1)\n",
    "y1 = df1['target']\n",
    "\n",
    "X2 = df2.drop(['Patient Group', 'target'], axis=1)\n",
    "y2 = df2['target']\n",
    "\n",
    "X3 = df3.drop(['Patient Group', 'target'], axis=1)\n",
    "y3 = df3['target']\n",
    "\n",
    "X4 = df4.drop(['Patient Group', 'target'], axis=1)\n",
    "y4 = df4['target']\n",
    "\n",
    "X5 = df5.drop(['Patient Group', 'target'], axis=1)\n",
    "y5 = df5['target']\n",
    "\n",
    "print(\"Data 1 Shape: \", X1.shape, y1.shape)\n",
    "print(\"Data 2 Shape: \", X2.shape, y2.shape)\n",
    "print(\"Data 3 Shape: \", X3.shape, y3.shape)\n",
    "print(\"Data 4 Shape: \", X4.shape, y4.shape)\n",
    "print(\"Data 5 Shape: \", X5.shape, y5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2d609",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5709ae06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 0.001, 'max_iter': 100, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Accuracy: 82.69%\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter': [100, 500, 1000]\n",
    "}\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Use GridSearchCV to search for the best hyperparameters\n",
    "grid = GridSearchCV(model, param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best Hyperparameters:', grid.best_params_)\n",
    "\n",
    "# Train the model on the training data with the best hyperparameters\n",
    "best_model = grid.best_estimator_\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "# Save the scaler to the \"Model_Saved\" folder\n",
    "joblib.dump(X_scaler, f\"Model_Saved/{name}_X_scaler.joblib\")\n",
    "\n",
    "# Save the model to the \"Model_Saved\" folder\n",
    "joblib.dump(best_model, f\"Model_Saved/{name}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f4b1f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to optimise and run Logistic Regression model, save model and scaler.\n",
    "def createLogReg(X, y, name):\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    # Define the parameter grid to search over\n",
    "    param_grid = {\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'max_iter': [100, 500, 1000]\n",
    "    }\n",
    "\n",
    "    # Create a Logistic Regression model\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Use GridSearchCV to search for the best hyperparameters\n",
    "    grid = GridSearchCV(model, param_grid=param_grid, cv=5)\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Print the best hyperparameters\n",
    "    print('Best Hyperparameters:', grid.best_params_)\n",
    "\n",
    "    # Train the model on the training data with the best hyperparameters\n",
    "    best_model = grid.best_estimator_\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "    # Save the scaler to the \"Model_Saved\" folder\n",
    "    joblib.dump(X_scaler, f\"Model_Saved/{name}_X_scaler.joblib\")\n",
    "\n",
    "    # Save the model to the \"Model_Saved\" folder\n",
    "    joblib.dump(best_model, f\"Model_Saved/{name}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b17e7006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 0.001, 'max_iter': 100, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Accuracy: 82.69%\n"
     ]
    }
   ],
   "source": [
    "createLogReg(X1, y1, 'Input_1_Target_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "375841a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8269230769230769\n"
     ]
    }
   ],
   "source": [
    "loaded_model = joblib.load(\"Model_Saved/Input_1_Target_1.joblib\")\n",
    "result = loaded_model.score(X_test_scaled, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25fa1079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 0.001, 'max_iter': 100, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Accuracy: 82.69%\n"
     ]
    }
   ],
   "source": [
    "createLogReg(X2, y2, 'Input_1_Target_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00cd2a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 0.001, 'max_iter': 100, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Accuracy: 82.69%\n"
     ]
    }
   ],
   "source": [
    "createLogReg(X3, y3, 'Input_1_Target_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d501c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 0.001, 'max_iter': 100, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Accuracy: 82.69%\n"
     ]
    }
   ],
   "source": [
    "createLogReg(X4, y4, 'Input_1_Target_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99ca048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 0.001, 'max_iter': 100, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Accuracy: 82.69%\n"
     ]
    }
   ],
   "source": [
    "createLogReg(X5, y5, 'Input_1_Target_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a8972",
   "metadata": {},
   "source": [
    "#### Logistic Regression model achieved same results for different target groups using input 1 ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8fc7a2",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d8cbfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.15%\n",
      "Best parameters: {'min_samples_split': 6, 'min_samples_leaf': 9, 'max_features': 'sqrt', 'max_depth': 14, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Create a Decision Tree Classifier model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Set up a parameter grid to search over\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': np.arange(3, 15),\n",
    "    'min_samples_split': np.arange(2, 10),\n",
    "    'min_samples_leaf': np.arange(1, 10),\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Perform a Randomized Search over the parameter grid\n",
    "search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=100, cv=5, random_state=42)\n",
    "search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and model\n",
    "best_params = search.best_params_\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "# Train the model on the training data\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "# Print out the best parameters\n",
    "print(\"Best parameters:\", search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db521a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDecTree(X, y):\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    # Create a Decision Tree Classifier model\n",
    "    model = DecisionTreeClassifier()\n",
    "\n",
    "    # Set up a parameter grid to search over\n",
    "    param_grid = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': np.arange(3, 15),\n",
    "        'min_samples_split': np.arange(2, 10),\n",
    "        'min_samples_leaf': np.arange(1, 10),\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }\n",
    "\n",
    "    # Perform a Randomized Search over the parameter grid\n",
    "    search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=100, cv=5, random_state=42)\n",
    "    search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get the best parameters and model\n",
    "    best_params = search.best_params_\n",
    "    best_model = search.best_estimator_\n",
    "\n",
    "    # Train the model on the training data\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "    # Print out the best parameters\n",
    "    print(\"Best parameters:\", search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc6d0c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.23%\n",
      "Best parameters: {'min_samples_split': 3, 'min_samples_leaf': 5, 'max_features': 'log2', 'max_depth': 14, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "createDecTree(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21b7e4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.22%\n",
      "Best parameters: {'min_samples_split': 6, 'min_samples_leaf': 9, 'max_features': 'log2', 'max_depth': 13, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "createDecTree(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8255477d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.67%\n",
      "Best parameters: {'min_samples_split': 7, 'min_samples_leaf': 9, 'max_features': 'auto', 'max_depth': 8, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "createDecTree(X3, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fd6bb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.63%\n",
      "Best parameters: {'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'log2', 'max_depth': 12, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "createDecTree(X4, y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06c56c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.62%\n",
      "Best parameters: {'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "createDecTree(X5, y5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160310ad",
   "metadata": {},
   "source": [
    "#### Decision Tree models did not perform as well as Logistic Regression ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685a0c30",
   "metadata": {},
   "source": [
    "## Random Forrest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c2666c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRandomForest(X, y): \n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 5, 10],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "\n",
    "    # Create a random forest model\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    search = GridSearchCV(model, param_grid=param_grid, cv=5)\n",
    "\n",
    "    # Fit the GridSearchCV object to the data\n",
    "    search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get the best parameters and model\n",
    "    best_params = search.best_params_\n",
    "    best_model = search.best_estimator_\n",
    "\n",
    "    # Train the model on the training data\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "    # Print out the best parameters\n",
    "    print(\"Best parameters:\", search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de80fc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.77%\n",
      "Best parameters: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "createRandomForest(X1, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24703eff",
   "metadata": {},
   "source": [
    "#### Random Forest model performed almost as well as Logistic Regression. Processing time was just under 3 minutes. ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e9f422",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34e7d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSVM(X, y):\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    # Create an SVM model with a linear kernel\n",
    "    model = SVC(kernel='linear')\n",
    "\n",
    "    # Set up a parameter grid to search over\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'gamma': [0.1, 1, 10, 100],\n",
    "    }\n",
    "\n",
    "    # Perform a Grid Search over the parameter grid\n",
    "    search = GridSearchCV(model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "    search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get the best parameters and model\n",
    "    best_params = search.best_params_\n",
    "    best_model = search.best_estimator_\n",
    "\n",
    "    # Train the model on the training data\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "    # Print the best parameters\n",
    "    print('Best Parameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd8fb4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.62%\n",
      "Best Parameters: {'C': 0.01, 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "createSVM(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0208c7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.98%\n",
      "Best Parameters: {'C': 0.01, 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "createSVM(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a84f943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.67%\n",
      "Best Parameters: {'C': 0.01, 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "createSVM(X3, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3b7e789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 36.84%\n",
      "Best Parameters: {'C': 0.01, 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "createSVM(X4, y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2aabb591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.62%\n",
      "Best Parameters: {'C': 0.01, 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "createSVM(X5, y5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bdf464",
   "metadata": {},
   "source": [
    "#### SVM outperformed Logistic Regression for target 1 but performed poorly for the other target groups. There was little to no processing time (1-4 seconds). ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea4f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
