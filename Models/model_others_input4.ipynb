{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52af95d2",
   "metadata": {},
   "source": [
    "# ML-diagnosis-of-esophageal-cancer\n",
    "## Supervised Machine Learning Model Selection\n",
    "## Input 4: Clinical data and protein ratios ### \n",
    "Authors: \n",
    "\n",
    "Date: 2023-03-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a334103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies & Installs\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa39aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input 1\n",
    "data = pd.read_csv('../Data_Cleaned/clinic_ratios.csv')\n",
    "\n",
    "# Reclassified: 1 vs 2,3,4 healthy vs all\n",
    "df1= data\n",
    "df1['target'] = df1['Patient Group'].map({'BE-HGD': 1, 'EAC': 1, 'BE': 1, 'BE-ID': 1, 'BE-LGD': 1, 'NSE': 0})\n",
    "df1 = df1[df1.target<2]\n",
    "\n",
    "# Reclassified: 2 vs 3&4 BE low vs BE-HGD & EAC\n",
    "df2 = data\n",
    "df2['target'] = df2['Patient Group'].map({'BE-HGD': 1, 'EAC': 1, 'BE': 0, 'BE-ID': 0, 'BE-LGD': 0, 'NSE': 2})\n",
    "df2 = df2[df2.target<2]\n",
    "\n",
    "# Reclassified: 2 vs 3 BE low vs BE-HGD\n",
    "df3 = data\n",
    "df3['target'] = df3['Patient Group'].map({'BE-HGD': 1, 'EAC': 2, 'BE': 0, 'BE-ID': 0, 'BE-LGD': 0, 'NSE': 2})\n",
    "df3 = df3[df3.target<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0c23c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 1 Shape:  (257, 28) (257,)\n",
      "Data 2 Shape:  (204, 28) (204,)\n",
      "Data 3 Shape:  (149, 28) (149,)\n"
     ]
    }
   ],
   "source": [
    "# Create X and y\n",
    "\n",
    "X1 = df1.drop(['Patient Group', 'target'], axis=1)\n",
    "y1 = df1['target']\n",
    "\n",
    "X2 = df2.drop(['Patient Group', 'target'], axis=1)\n",
    "y2 = df2['target']\n",
    "\n",
    "X3 = df3.drop(['Patient Group', 'target'], axis=1)\n",
    "y3 = df3['target']\n",
    "\n",
    "print(\"Data 1 Shape: \", X1.shape, y1.shape)\n",
    "print(\"Data 2 Shape: \", X2.shape, y2.shape)\n",
    "print(\"Data 3 Shape: \", X3.shape, y3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1833a0ce",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e819e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d1f6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to optimise and run Logistic Regression model, save model and scaler.\n",
    "def createLogReg(X, y, name):\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    # Define the parameter grid to search over\n",
    "    param_grid = {\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'max_iter': [100, 500, 1000]\n",
    "    }\n",
    "\n",
    "    # Create a Logistic Regression model\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Use GridSearchCV to search for the best hyperparameters\n",
    "    grid = GridSearchCV(model, param_grid=param_grid, cv=5)\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Print the best hyperparameters\n",
    "    print('Best Hyperparameters:', grid.best_params_)\n",
    "\n",
    "    # Train the model on the training data with the best hyperparameters\n",
    "    best_model = grid.best_estimator_\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "    # Save the scaler to the \"Model_Saved\" folder\n",
    "    joblib.dump(X_scaler, f\"Model_Saved/{name}_X_scaler.joblib\")\n",
    "\n",
    "    # Save the model to the \"Model_Saved\" folder\n",
    "    joblib.dump(best_model, f\"Model_Saved/{name}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12451ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 10, 'max_iter': 100, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Accuracy: 78.85%\n"
     ]
    }
   ],
   "source": [
    "createLogReg(X1, y1, 'Input_4_Target_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bfa74a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 0.001, 'max_iter': 100, 'penalty': 'none', 'solver': 'saga'}\n",
      "Accuracy: 78.85%\n"
     ]
    }
   ],
   "source": [
    "createLogReg(X2, y2, 'Input_4_Target_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bad3494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 10, 'max_iter': 100, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Accuracy: 78.85%\n"
     ]
    }
   ],
   "source": [
    "createLogReg(X3, y3, 'Input_4_Target_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9f5d64",
   "metadata": {},
   "source": [
    "#### Logistic Regression model achieved same results for different target groups using input 4. Accuracy score 78.85% ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f57c14",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "828ec00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDecTree(X, y):\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    # Create a Decision Tree Classifier model\n",
    "    model = DecisionTreeClassifier()\n",
    "\n",
    "    # Set up a parameter grid to search over\n",
    "    param_grid = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': np.arange(3, 15),\n",
    "        'min_samples_split': np.arange(2, 10),\n",
    "        'min_samples_leaf': np.arange(1, 10),\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }\n",
    "\n",
    "    # Perform a Randomized Search over the parameter grid\n",
    "    search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=100, cv=5, random_state=42)\n",
    "    search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get the best parameters and model\n",
    "    best_params = search.best_params_\n",
    "    best_model = search.best_estimator_\n",
    "\n",
    "    # Train the model on the training data\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "    # Print out the best parameters\n",
    "    print(\"Best parameters:\", search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7deb0355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.92%\n",
      "Best parameters: {'min_samples_split': 9, 'min_samples_leaf': 9, 'max_features': 'sqrt', 'max_depth': 5, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "createDecTree(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5dae757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.22%\n",
      "Best parameters: {'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 14, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "createDecTree(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb6398a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.67%\n",
      "Best parameters: {'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 14, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "createDecTree(X3, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4be6fa",
   "metadata": {},
   "source": [
    "#### Decision Tree models achieved performance standard  target 1 only.####\n",
    "Target 1: 76.92%\n",
    "Target 2: 51.22%\n",
    "Target 3: 66.67% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea598b9",
   "metadata": {},
   "source": [
    "## Random Forrest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64359dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRandomForest(X, y): \n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 5, 10],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "\n",
    "    # Create a random forest model\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    search = GridSearchCV(model, param_grid=param_grid, cv=5)\n",
    "\n",
    "    # Fit the GridSearchCV object to the data\n",
    "    search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get the best parameters and model\n",
    "    best_params = search.best_params_\n",
    "    best_model = search.best_estimator_\n",
    "\n",
    "    # Train the model on the training data\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "    # Print out the best parameters\n",
    "    print(\"Best parameters:\", search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fc96678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.77%\n",
      "Best parameters: {'max_depth': 5, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "createRandomForest(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e75be150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.22%\n",
      "Best parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "createRandomForest(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f50eced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.67%\n",
      "Best parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "createRandomForest(X3, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc10dbf",
   "metadata": {},
   "source": [
    "#### Random Forest model preformed slightly worse than Decision Tree model overall. It achieved performance standard with target 1 and 3. Processing time was nearly 5 minutes ####\n",
    "Target 1: 80.77%\n",
    "Target 2: 51.22%\n",
    "Target 3: 76.67%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79ddd0",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a03eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSVM(X, y, name):\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    # Create an SVM model with a linear kernel\n",
    "    model = SVC(kernel='linear')\n",
    "\n",
    "    # Set up a parameter grid to search over\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'gamma': [0.1, 1, 10, 100],\n",
    "    }\n",
    "\n",
    "    # Perform a Grid Search over the parameter grid\n",
    "    search = GridSearchCV(model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "    search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get the best parameters and model\n",
    "    best_params = search.best_params_\n",
    "    best_model = search.best_estimator_\n",
    "\n",
    "    # Train the model on the training data\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "    # Print the best parameters\n",
    "    print('Best Parameters:', best_params)\n",
    "    \n",
    "    # Save the scaler to the \"Model_Saved\" folder\n",
    "    joblib.dump(X_scaler, f\"Model_Saved/{name}_X_scaler.joblib\")\n",
    "\n",
    "    # Save the model to the \"Model_Saved\" folder\n",
    "    joblib.dump(best_model, f\"Model_Saved/{name}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1e6d768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.62%\n",
      "Best Parameters: {'C': 0.1, 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "createSVM(X1, y1, 'SVM_input_4_target_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0aaef9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.85%\n",
      "Best Parameters: {'C': 0.01, 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "createSVM(X2, y2, 'SVM_input_4_target_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b75a597e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.67%\n",
      "Best Parameters: {'C': 0.01, 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "createSVM(X3, y3, 'SVM_input_4_target_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca488e",
   "metadata": {},
   "source": [
    "#### SVM outperformed Logistic Regression for target 1, performed poorly for target 2, and achieved performance standard for target 3. There was little to no processing time (1-4 seconds). ####\n",
    "Target 1: 84.62%\n",
    "Target 2: 65.85%\n",
    "Target 3: 76.67%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ab0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
